# LLM Code Generation and Test Analysis

This repository contains two main projects:
1.  **Solution Generation:** Scripts to generate Python solutions from the HumanEval dataset using various LLMs (Llama, Deepseek).
2.  **Test Coverage Analysis:** Scripts to measure test coverage, use an LLM to improve it, and perform fault detection.

---

## Part 1: Solution Generation (Exercise 1)

This section details the process of generating the solution files from the HumanEval dataset.

### 1.1 Source Data
* `tasks.jsonl`: The dataset extracted from the [HumanEval GitHub repo](https://github.com/openai/human-eval).

### 1.2 Solution Generation Scripts

**Llama (Local)**
* `generate_llm.py`: Main code to generate output from a local Llama model using `ollama`.
* **Usage:**
    1.  You may need to install `ollama` locally before running this file.
    2.  Change the prompts and file names in the script as needed.
    3.  Run: `python generate_llm.py`

**Deepseek (Hugging Face)**
* `generate_solutions.py`: Main code to generate an output `.jsonl` file using a Deepseek model from Hugging Face.
* **Usage:**
    ```bash
    python generate_solutions.py \
        --model_name "deepseek-ai/deepseek-coder-1.3b-instruct" \
        --tasks_file "tasks.jsonl" \
        --output_file "results/deepseek_cot_output.jsonl" \
        --prompt_style "cot"
    ```
* You can change the `--prompt_style` to "self-debug" or other strategies.
* **Note:** I used the 1.3B parameter model due to laptop compute capacity. The output `.jsonl` files are stored in the `/results/` directory.

### 1.3 Utility Scripts
* `clean_results.py`: Cleans the generated `.jsonl` files by removing extra text before the code blocks to prevent syntax errors.
* `check.py`: A utility to check if the generated JSON files have proper code solutions for the given prompts.

### 1.4 Results (pass@1)

The `pass@1` metric was calculated by running the generated solutions against the benchmark tests.

<img width="427" height="202" alt="image" src="https://github.com/user-attachments/assets/51ed7b23-33db-4823-9f77-fa7e36c8899b" />
<img width="333" height="260" alt="image" src="https://github.com/user-attachments/assets/110945d1-f5ab-49a3-b375-5cc883dc5db0" />

---

## Part 2: Test Coverage & Fault Detection Analysis (Exercise 2)

This section details the process of analyzing the test coverage of the LLM-generated solutions and improving it.

### 2.1 Setup

1.  **Create a Virtual Environment** (Recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

2.  **Install Dependencies:**
    Create a `requirements.txt` file with the content below and run `pip install -r requirements.txt`.

    **`requirements.txt`:**
    ```
    pytest
    pytest-cov
    pandas
    tabulate
    ```

3.  **Prepare Solution File:**
    This analysis requires a single solution file. Rename your chosen LLM-generated output file (e.g., `results/deepseek_cot_output.jsonl`) to `completions.jsonl` and place it in the root directory.

### 2.2 Step 1: Baseline Coverage (Assignment Part 1)

This script runs the benchmark tests (from `tasks.jsonl`) against the LLM solutions (from `completions.jsonl`) and generates the baseline coverage table.

* **Run Command:**
    ```bash
    python run_coverage.py
    ```
* This will output the Markdown table required for the report.

### 2.3 Step 2: Coverage Improvement (Assignment Part 2)

This script runs the benchmark tests *plus* new LLM-generated tests (from the `new_tests_*.py` files) to measure the improvement in coverage.

* **Run Command for `HumanEval/12`:**
    ```bash
    python run_cumulative_coverage.py HumanEval/12 new_tests_h12_iter1.py
    ```

* **Run Command for `HumanEval/100`:**
    ```bash
    python run_cumulative_coverage.py HumanEval/100 new_tests_h100_iter1.py
    ```

* **View HTML Report:** After running, you can view a detailed, line-by-line report by opening the `htmlcov/index.html` file in your browser.

### 2.4 Step 3: Fault Detection (Assignment Part 3)

This part of the assignment was a manual analysis described in the final PDF report. The process involved:
1.  Injecting a bug into the temporary `temp_solution.py` file (which is generated by the `run_cumulative_coverage.py` script).
2.  Re-running the tests from Step 2 (e.g., `python run_cumulative_coverage.py HumanEval/12 new_tests_h12_iter1.py`).
3.  Confirming that the new, LLM-generated tests failed, thus "catching" the bug.

### 2.5 Analysis File Structure

<img width="717" height="287" alt="Screenshot 2025-11-09 at 12 09 23â€¯AM" src="https://github.com/user-attachments/assets/5c4f25cb-eaef-46c3-8339-568c0b9edc97" />

